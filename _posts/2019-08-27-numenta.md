---
layout: post
title: Numenta Stuff
subtitle: a disorganized collection of bio-inspired ML notes
css: css/prt.css
---

Before jumping in, here is a [good set of visualizations](https://htm-community.github.io/sanity/)

##Hierachical temporal memory

 is an unsupervised learning model directly inspired by neuroscientific research and results.  
This is a really impressive enterprise for a few reasons, only a couple I will enumerate here.
 Solving machine learning computations one-to-one with the brain's underlying computational functionality will be both energy efficient and massively adaptive to new signals; the brain is an inherently online system. 
The implied consequence here also is we will have solved human cognition, or at least human neocortical algorithmic functionality, which is massively impressive in its own right. 

The MO here is humans cognate in the fourth dimension, time.
 as such, the model should be intrinsically sequence-based.
 The model behaves as an adapting sequence predictor with intralayer feedback to inform the positivity or misclassification of the signal on a per-predictor basis
There are some amazing online visuals of the model in actions, which is really something more papers and proposals should address if they seek true adoptability.  
I will include them later in this post
There are a few fundamental concepts that form the foundation for HTM.
One of these is termed sparsity.  
Think about it as the converse to density.
  Where one desires very few points of salience in a structure.   
The term is Sparse distributed representation, referred to in the research by the corresponding acronym, SDR.
 The idea is this: if there are only a few places where things are active, against the context of a bunch of inactivity, IF another representation comes through that has activity in similar locations, the probability of that being a false positive is directly related to 1 over the number of units choose the number of active units. 
We use the structure to imply correctness for us! This is pretty revolutionary, because it gets rid of the need for labelled data, because the data just labels itself instead, by virtue of existing.  

if we have an array like this

```C
00000000 00000000 00000000 00000000 
00000000 00000000 00000000 00000000 
00000000 01000000 00000000 00000000 
00000000 00000000 00000000 00000000 
00000000 00000000 00000000 00000000 
00000000 00000000 00000000 00000000 
00000000 00000000 00100000 00000000 
00000000 00000000 00000000 00000000 
```

there are exactly two on positions within this collection of 256 units.
These positions are 73 and 199, using zero-based indexing.
If another array were to appear with the exact configuration of this, from a uniform random distribution of activiations....
Well the chance of this is very low.  
If you are familiar with combinatorics at all, applying the choose function here will help up home in on the answer.
Basically, embed latex here.  n choose w is the number of combinations of w on bits in an array of size n.  
1 over that number is the chance that a corresponding signal with the exact same representation is just noise. 

The fact that we solve for many issues at once is quite remarkable, and it is also a testament to the ingenuity of nature.  
There is a tremendous amount of available representation in just 256 bits.
There is also a massive guarantee that the signal it represents is a true positive.  
It is also very computationally conservative; since we only are dealing with on and off states here, it can be effectively encoded through computationally efficient bit-wise operations.
In fact, the encoding can be made effectively more memory dense as well, by an order of 8 times!
The encoding here is actually just 32 bytes


```C
0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 
0x00 0x40 0x00 0x00 0x00 0x00 0x00 0x00 
0x00 0x00 0x00 0x00 0x00 0x00 0x00 0x00 
0x00 0x00 0x20 0x00 0x00 0x00 0x00 0x00 
```


The 8 times memory efficiency is illustrated here:

```C
char[256] = 
{ 0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0,
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0,
  0, 1, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0,  
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 1, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0, 
  0, 0, 0, 0, 0, 0, 0, 0
};
```

That was probably a lot to scroll through, but I hope you get the idea.
Now, that isn't even considering if we were to have integer representations, which would be 32 times larger than the earlier version. 
Floating point would also confer the same drawbacks memory wise, but it is also more computationally demanding of an operative data primitive.
If you have any idea as to how IEEE floating point computation is, you will immediately understand the desireabilitiy of simple data primitives


